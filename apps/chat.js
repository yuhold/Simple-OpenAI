import plugin from '../../../lib/plugins/plugin.js'
import fetch from 'node-fetch'
import { HttpsProxyAgent } from 'https-proxy-agent' // å¼•å…¥ä»£ç†æ¨¡å—
import Config from '../model/config.js'

const cfg = new Config()
const historyMap = new Map()

export class OpenAIChat extends plugin {
    constructor() {
        const config = cfg.getConfig()
        const escPrefix = config.prefix.replace(/([.*+?^=!:${}()|[\]/\\])/g, "\\$1")

        super({
            name: 'Simple-OpenAI',
            dsc: 'OpenAIå¯¹è¯æ’ä»¶(æ”¯æŒä»£ç†)',
            event: 'message',
            priority: 5000,
            rule: [
                { reg: `^${escPrefix}`, fnc: 'chat' },
                { reg: '^#é‡ç½®å¯¹è¯$', fnc: 'resetChat' }
            ]
        })
    }

    getChatId(e) { return e.isGroup ? `group:${e.group_id}` : `user:${e.user_id}` }

    async resetChat(e) {
        historyMap.delete(this.getChatId(e))
        await e.reply('ğŸ—‘ï¸ è®°å¿†å·²æ¸…é™¤ï¼Œå¼€å¯æ–°è¯é¢˜ã€‚')
    }

    async chat(e) {
        const config = cfg.getConfig()
        
        if (!config.apiKey) {
            await e.reply('è¯·å…ˆåœ¨é”…å·´æ’ä»¶ä¸­é…ç½® API Keyã€‚')
            return
        }

        let prompt = e.msg.replace(new RegExp(`^${config.prefix}`), '').trim()
        if (!prompt) return

        const chatId = this.getChatId(e)
        let history = historyMap.get(chatId) || []
        history.push({ role: "user", content: prompt })

        const maxHistory = config.historyCount || 10
        if (history.length > maxHistory) history = history.slice(-maxHistory)

        try {
            // --- ä»£ç†é…ç½®æ ¸å¿ƒé€»è¾‘ ---
            let fetchOptions = {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${config.apiKey}`
                },
                body: JSON.stringify({
                    model: config.model,
                    messages: [
                        { role: "system", content: config.systemPrompt },
                        ...history
                    ],
                    temperature: 0.7
                })
            }

            // å¦‚æœé…ç½®äº†ä»£ç†åœ°å€ï¼Œæ³¨å…¥ agent
            if (config.proxyUrl) {
                fetchOptions.agent = new HttpsProxyAgent(config.proxyUrl)
            }
            // ---------------------

            const response = await fetch(config.baseUrl, fetchOptions)

            if (!response.ok) {
                const errText = await response.text()
                console.error(`[OpenAI Error] ${response.status}: ${errText}`)
                
                // å‡ºé”™å›æ»š
                history.pop()
                historyMap.set(chatId, history)
                
                await e.reply(`è¯·æ±‚å¤±è´¥: ${response.status}\nè¯·æ£€æŸ¥API Keyæˆ–ç½‘ç»œè¿æ¥(ä»£ç†)ã€‚`)
                return
            }

            const data = await response.json()
            
            if (data.choices && data.choices.length > 0) {
                const replyContent = data.choices[0].message.content.trim()
                history.push({ role: "assistant", content: replyContent })
                historyMap.set(chatId, history)
                await e.reply(replyContent, true)
            } else {
                history.pop()
                historyMap.set(chatId, history)
                await e.reply('æ¥å£è¿”å›ç©ºå†…å®¹ã€‚')
            }

        } catch (error) {
            console.error('[OpenAI Plugin Error]', error)
            history.pop()
            historyMap.set(chatId, history)

            if (error.code === 'ETIMEDOUT' || error.type === 'system') {
                await e.reply('è¿æ¥è¶…æ—¶ï¼è¯·æ£€æŸ¥æ˜¯å¦å¡«å†™äº†æ­£ç¡®çš„ HTTP ä»£ç†åœ°å€ã€‚')
            } else {
                await e.reply(`å‘ç”Ÿé”™è¯¯: ${error.message}`)
            }
        }
    }
}
